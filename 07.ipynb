{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確率ロボティクス2017第7回\n",
    "\n",
    "上田隆一\n",
    "\n",
    "2017年10月18日@千葉工業大学\n",
    "\n",
    "## 今日やること\n",
    "\n",
    "* 有限マルコフ決定過程\n",
    "\n",
    "## 有限マルコフ決定過程\n",
    "\n",
    "* finite Markov decision processes (finite MDPs, 有限MDP)\n",
    "  * ロボットの行動を考えるときによく使う\n",
    "    * 行動をするに当たって何が「最適」なのか？\n",
    "    * 最適にするにはロボットがどうすべきか？\n",
    "  * 昔ながらの（今も使われる）経路計画の定式化も有限MDPの部分問題\n",
    "  \n",
    "## 有限MDPの考え方\n",
    "\n",
    "* ロボット（環境）の状態は制御出力により遷移\n",
    "  * これまでの話と同じ\n",
    "  * 雑音が伴うのも同じ\n",
    "* 状態をエージェントは観察できる\n",
    "  * これまでとは違う\n",
    "* 「こういう状態になったら終わり」という状態が存在\n",
    "  * 「終端状態」 \n",
    "  * 例\n",
    "    * 移動ロボットが目的地に入った状態\n",
    "    * ロボットが川に落ちた\n",
    "* 状態遷移にはコストやペナルティーが伴う\n",
    "  * コスト: 時間、電力消費、燃料消費等\n",
    "  * ペナルティー: ルール違反、危険な行為に対して\n",
    "  \n",
    "## 有限MDPの定式化\n",
    "\n",
    "### 状態\n",
    "\n",
    "* 離散状態の集合を考える\n",
    "  * $\\mathcal{S}= \\{s^{(i)} | i=1,2,\\dots,N \\}$\n",
    "* 状態が連続の場合は離散化\n",
    "  * $\\mathcal{X}$を区切る \n",
    "\n",
    "### 終端状態\n",
    "\n",
    "* $s_\\text{f} \\in \\mathcal{S}_\\text{f} \\subset \\mathcal{S}$\n",
    "* この状態に到達するとそれ以上時間が進まない\n",
    "* タスクの終わり \n",
    "\n",
    "### 制御出力（行動）\n",
    "\n",
    "* 有限個の行動の集合から選択\n",
    "  * $\\mathcal{A} = \\{ a^{(j)} | j = 1,2,\\dots,M \\}$\n",
    "* 連続系の$\\mathcal{U}$から何種類か選んで有限個に\n",
    "* この資料では$s_i,a_j$と書くときは時刻を表すことにします\n",
    "\n",
    "### 状態遷移\n",
    "\n",
    "* $s' \\sim P(s' | s,a)$ \n",
    "  * $s,s'$はそれぞれ遷移前、遷移後の状態を示す\n",
    "* $P(s' | s,a)$を$\\mathcal{P}^a_{ss'}$と表記しましょう\n",
    "  * $s \\in \\mathcal{S} - \\mathcal{S}_\\text{f}$から$a \\in \\mathcal{A}$を選択した時、$s' \\in \\mathcal{S}$に遷移する確率\n",
    "  \n",
    "### 状態遷移の評価\n",
    "\n",
    "* $R(s,a,s') \\in \\Re$\n",
    "  * 評価は常に実数で行われる\n",
    "* 時刻に依存しないと仮定しましょう\n",
    "* 評価軸がいくつあっても実数に置き換えて一元化\n",
    "  * 例: 移動ロボットが水たまりを踏んだら1回につき3分のペナルティー、等\n",
    "* $R(s,a,s')$を$\\mathcal{R}^a_{ss'}$と表記\n",
    "\n",
    "### 終端状態の評価\n",
    "\n",
    "* どの終端状態に行くかも評価の対象\n",
    "* 終端状態の価値\n",
    "  * $V(s)\\in \\Re$ where $\\forall s \\in \\mathcal{S}_\\text{f}$\n",
    "  * この値も$\\mathcal{R}^a_{ss'}$と同じ評価軸の上にある\n",
    "  \n",
    "### 行動の評価\n",
    "\n",
    "* ある状態からスタートして、ある終端状態に入った時の評価の和\n",
    "* 状態遷移と行動が$s_0,a_1,s_1,a_2,\\dots,s_T$だったときの評価:\n",
    "  * $J(s_{0:T},a_{1:T})$ $ = \\mathcal{R}^{a_1}_{s_0s_1} +\\mathcal{R}^{a_2}_{s_1s_2} +\\mathcal{R}^{a_3}_{s_2s_3} + \\dots +\\mathcal{R}^{a_T}_{s_{T-1}s_T} + V(s_T)$\n",
    "$ = \\sum_{t=1}^T \\mathcal{R}^{a_t}_{s_{t-1}s_t} + V(s_T)  $\n",
    "* $J(s_{0:T},a_{1:T})$は毎回違う\n",
    "  * $\\mathcal{P}^a_{ss'}$で、事後の状態$s'$がばらつく\n",
    "  \n",
    "### 有限MDPでの最適な行動\n",
    "\n",
    "* $J(s_{0:T},a_{1:T})$の期待値が最大になる行動決定則\n",
    "  * 毎回、事後状態がばらつくのにどうやって求めるの？\n",
    "  * 結構難しい\n",
    "    * 識別の問題なら判断は1回\n",
    "    * 行動決定の場合は終端状態に至るまで何度も判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
