{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確率ロボティクス2017第7回\n",
    "\n",
    "上田隆一\n",
    "\n",
    "2017年10月18日@千葉工業大学\n",
    "\n",
    "## 今日やること\n",
    "\n",
    "* 有限マルコフ決定過程\n",
    "\n",
    "## 有限マルコフ決定過程\n",
    "\n",
    "* finite Markov decision processes (finite MDPs, 有限MDP)\n",
    "  * ロボットの行動を考えるときによく使う\n",
    "    * 行動をするに当たって何が「最適」なのか？\n",
    "    * 最適にするにはロボットがどうすべきか？\n",
    "  * 昔ながらの（今も使われる）経路計画の定式化も有限MDPの部分問題\n",
    "  \n",
    "## 有限MDPの考え方\n",
    "\n",
    "* ロボット（環境）の状態は制御出力により遷移\n",
    "  * これまでの話と同じ\n",
    "  * 雑音が伴うのも同じ\n",
    "* 状態をエージェントは観察できる\n",
    "  * これまでとは違う\n",
    "* 「こういう状態になったら終わり」という状態が存在\n",
    "  * 「終端状態」 \n",
    "  * 例\n",
    "    * 移動ロボットが目的地に入った状態\n",
    "    * ロボットが川に落ちた\n",
    "* 状態遷移にはコストやペナルティーが伴う\n",
    "  * コスト: 時間、電力消費、燃料消費等\n",
    "  * ペナルティー: ルール違反、危険な行為に対して\n",
    "  \n",
    "## 有限MDPの定式化\n",
    "\n",
    "### 状態\n",
    "\n",
    "* 離散状態の集合を考える\n",
    "  * $\\mathcal{S}= \\{s^{(i)} | i=1,2,\\dots,N \\}$\n",
    "* 状態が連続の場合は離散化\n",
    "  * $\\mathcal{X}$を区切る \n",
    "\n",
    "### 終端状態\n",
    "\n",
    "* $s_\\text{f} \\in \\mathcal{S}_\\text{f} \\subset \\mathcal{S}$\n",
    "* この状態に到達するとそれ以上時間が進まない\n",
    "* タスクの終わり \n",
    "\n",
    "### 制御出力（行動）\n",
    "\n",
    "* 有限個の行動の集合から選択\n",
    "  * $\\mathcal{A} = \\{ a^{(j)} | j = 1,2,\\dots,M \\}$\n",
    "* 連続系の$\\mathcal{U}$から何種類か選んで有限個に\n",
    "* この資料では$s_i,a_j$と書くときは時刻を表すことにします\n",
    "\n",
    "### 状態遷移\n",
    "\n",
    "* $s' \\sim P(s' | s,a)$ \n",
    "  * $s,s'$はそれぞれ遷移前、遷移後の状態を示す\n",
    "* $P(s' | s,a)$を$\\mathcal{P}^a_{ss'}$と表記しましょう\n",
    "  * $s \\in \\mathcal{S} - \\mathcal{S}_\\text{f}$から$a \\in \\mathcal{A}$を選択した時、$s' \\in \\mathcal{S}$に遷移する確率\n",
    "  \n",
    "### 状態遷移の評価\n",
    "\n",
    "* $R(s,a,s') \\in \\Re$\n",
    "  * 評価は常に実数で行われる\n",
    "* 時刻に依存しないと仮定しましょう\n",
    "* 評価軸がいくつあっても実数に置き換えて一元化\n",
    "  * 例: 移動ロボットが水たまりを踏んだら1回につき3分のペナルティー、等\n",
    "* $R(s,a,s')$を$\\mathcal{R}^a_{ss'}$と表記\n",
    "\n",
    "### 終端状態の評価\n",
    "\n",
    "* どの終端状態に行くかも評価の対象\n",
    "* 終端状態の価値\n",
    "  * $V(s)\\in \\Re$ where $\\forall s \\in \\mathcal{S}_\\text{f}$\n",
    "  * この値も$\\mathcal{R}^a_{ss'}$と同じ評価軸の上にある\n",
    "  \n",
    "### 行動の評価\n",
    "\n",
    "* ある状態からスタートして、ある終端状態に入った時の評価の和\n",
    "* 状態遷移と行動が$s_0,a_1,s_1,a_2,\\dots,s_T$だったときの評価:\n",
    "  * $J(s_{0:T},a_{1:T})$ $ = \\mathcal{R}^{a_1}_{s_0s_1} +\\mathcal{R}^{a_2}_{s_1s_2} +\\mathcal{R}^{a_3}_{s_2s_3} + \\dots +\\mathcal{R}^{a_T}_{s_{T-1}s_T} + V(s_T)$\n",
    "$ = \\sum_{t=1}^T \\mathcal{R}^{a_t}_{s_{t-1}s_t} + V(s_T)  $\n",
    "* $J(s_{0:T},a_{1:T})$は毎回違う\n",
    "  * $\\mathcal{P}^a_{ss'}$で、事後の状態$s'$がばらつく\n",
    "  \n",
    "### 有限MDPでの最適な行動\n",
    "\n",
    "* $J(s_{0:T},a_{1:T})$の期待値が最大になる行動決定則\n",
    "  * 毎回、事後状態がばらつくのにどうやって求めるの？\n",
    "  * 結構難しい\n",
    "    * 識別の問題なら判断は1回\n",
    "    * 行動決定の場合は終端状態に至るまで何度も判断\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例題\n",
    "\n",
    "次のような簡単な問題を扱います。\n",
    "\n",
    "3x3の碁盤の世界を準備します。ロボットはマス目のどこかに存在し、1歩で上下左右に1マス移動できます。ただし、10%の確率で移動に失敗し、元のマス目に戻ります。右上をゴールにして、あるマス目をスタート地点とするとき、何歩でゴールに達するかを、全マス目に対して計算しましょう。ただし、この世界は壁に囲まれていて、壁を超えて移動しようとすると元のマスに戻されます。\n",
    "\n",
    "\n",
    "#### 準備\n",
    "\n",
    "次のような変数を準備します。\n",
    "\n",
    "* costs: 各マスをスタートにしたときのゴールまでの歩数を記録する2次元のリスト\n",
    "* goal: ゴールの座標。ゴールは2つ以上でも良いが、この問題では1個だけ\n",
    "* actions: ロボットが選択できる行動のリスト\n",
    "\n",
    "valuesの初期値はデタラメで大丈夫ですが、ゴールに相当するマス目のものだけ正解の0にセットしておきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## 状態のマルコフ性と最適性の原理\n",
    "\n",
    "* $\\mathcal{P}^a_{ss'}, \\mathcal{R}^a_{ss'}$が時不変\n",
    "  * 違う時刻に同じ状態に来たら、同じ行動をとれば同じ確率分布で次の状態に遷移し、同じように評価される\n",
    "* これが成り立つと・・・\n",
    "  * ある試行の状態遷移$s_0,s_1,s_2,\\dots,s_N$のうち、どこを初期状態にしても、$J(s_{0:T},a_{1:T})$の期待値を最大にする問題は、同じ問題になる。\n",
    "  \n",
    "## 状態の価値\n",
    "\n",
    "* $J(s_{0:T},a_{1:T})$の期待値の最大値を考えてみる \n",
    "  * 先ほどの議論から、$J(s_{0:T},a_{1:T})$の期待値の最大値が存在すると仮定すると、初期状態だろうがなかろうが、ある状態$s$から先の行動決定を考えると、この期待値の最大値が存在するし、それは、その状態$s$の関数となる\n",
    "  * これをその状態の（エージェントが最適な行動を取った時の）「価値」と言う\n",
    "* 終端状態の価値$V(s) ( s \\in \\mathcal{S}_\\text{f})$を拡張して、状態空間全体の関数: $V^*: \\mathcal{S} \\to \\Re$とする\n",
    "  * $V^*$: 最適状態価値関数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 価値の性質（ベルマン方程式）\n",
    "\n",
    "* $V^* (s) = \\max_{a\\in\\mathcal{A}} \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\left[ V^*(s') + \\mathcal{R}_{ss'}^a \\right] $\n",
    "  * もし最適状態価値関数が存在するなら、ある状態$\\forall s \\in \\mathcal{S}-\\mathcal{S}_\\text{f}$の値$V^*(s)$は、以下の値を最大にする行動$a \\in \\mathcal{A}$を選んだ時の値になる\n",
    "    * $s$から遷移した先の値$V^*(s')$と、その状態遷移に対する評価$\\mathcal{R}_{ss'}^a$を足した値を考える。遷移先は確率的にばらつくので、この値について$\\mathcal{P}_{ss'}^a$で重み付き平均を求めたもの。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 価値の計算（価値反復）\n",
    "\n",
    "* ある状態$s$の$V^*(s)$だけ分からない。あとの状態の$V^*$の値は分かるという、都合の良い>状況を考えてみましょう。\n",
    "  * 価値の計算式はベルマン方程式そのものとなる\n",
    "    * $V^* (s) =\\max_{a\\in\\mathcal{A}} \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\left[ V^*(s') + \\mathcal{R}_{ss'}^a \\right] $\n",
    "* 価値反復アルゴリズム\n",
    "  * 終端状態の価値以外を適当に初期化して、上の計算を$\\forall s \\in \\mathcal{S} - \\mathcal{S}_\\text{f}$に対して何度も計算していく\n",
    "  * 終端状態に近い状態から$V^* (s)$の値が収束していく "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最適方策\n",
    "\n",
    "* 最適状態価値関数が分かっていると、ベルマン方程式を成立させるための行動が求まる\n",
    "  * 最適な行動: $\\text{argmax}_a \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a \\left[ V^*(s') + \\mathcal{R}_{ss'}^a \\right] $\n",
    "* 「最適方策」\n",
    "  * $\\pi^*: \\mathcal{S}-\\mathcal{S}_\\text{f} \\to \\mathcal{A}$\n",
    "    * 最適方策は状態と行動を対にしたリストとなる\n",
    "    * 時刻は全く関係ない\n",
    "    * $\\pi^*$を知っているロボットは、置き直されたりタスクを妨害されたりしても、その後に自身の状態を知覚できれば、最適な行動をとることが可能"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
