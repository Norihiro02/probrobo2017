{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確率ロボティクス2017第10回\n",
    "\n",
    "上田隆一\n",
    "\n",
    "2017年11月22日@千葉工業大学"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今日やること\n",
    "\n",
    "* 前回の続き（Q学習）\n",
    "* 連続空間での強化学習について少しだけ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 連続空間での強化学習\n",
    "\n",
    "* 今までの話は離散状態が前提\n",
    "* 連続空間も考えなければいけないことがある\n",
    "  * ロボットは連続空間にいる\n",
    "  * 離散状態の数が膨大になる\n",
    "* 求める価値関数、方策\n",
    "  * 連続状態空間$\\mathcal{X}$に対して・・・ \n",
    "  * $V^*: \\mathcal{X} \\to \\Re$\n",
    "  * $\\pi^*: \\mathcal{X} \\to \\mathcal{A}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手法\n",
    "\n",
    "* 近似（強化学習をこじらせると関数近似に流れ着く）\n",
    "  * 動径基底関数\n",
    "  * タイルコーディング（離散的）\n",
    "  * 人工ニューラルネットワーク（ディープニューラルネットワーク）\n",
    "    * TD-Gammon, DQN, A3C\n",
    "* アクター・クリティック（actor-critic）  \n",
    "  * 方策$\\pi$の実装と価値関数$V$の実装を分けることができて柔軟\n",
    "  * A3Cはアクター・クリティック\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
