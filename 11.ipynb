{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 確率ロボティクス2017第11回\n",
    "\n",
    "上田隆一\n",
    "\n",
    "2017年11月29日@千葉工業大学\n",
    "\n",
    "## 今日やること\n",
    "\n",
    "* POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POMDP\n",
    "\n",
    "* 部分観測マルコフ決定過程\n",
    "* 状態が自明でない場合のマルコフ決定過程\n",
    "  * センサ情報から得られるが、部分的\n",
    "* ほぼ、自律型ロボットの問題と等価\n",
    "  * 状態が分かっているものをロボットと言うかどうか\n",
    "* 他者がいるとさらに問題が難しくなるが、POMDPでは考えない/扱えない\n",
    "\n",
    "\n",
    "## 定式化\n",
    "\n",
    "* 状態が直接観測できず、観測から間接的に分かる\n",
    "  * 要は自己位置推定の問題に行動決定をくっつけたもの\n",
    "* 各シンボルの定義\n",
    "  * 時刻: $\\mathcal{T} = \\{t | t=0,1,2,\\dots,T\\}$\n",
    "  * 状態: $\\mathcal{S} = \\{s_i | i=1,2,3,\\dots,N\\}$\n",
    "    * どんな状態があるかはknown、どの状態にいるかはunknown\n",
    "  * 行動: $\\mathcal{A} = \\{a_j | j=1,2,3,\\dots,M\\}$\n",
    "  * 状態遷移: $\\mathcal{P}_{ss'}^a$\n",
    "  * 報酬: $\\mathcal{R}_{ss'}^a$\n",
    "  * 観測: $\\mathcal{O}$スイカ割り\n",
    "\n",
    "## 問題の例\n",
    " \n",
    "* スイカ割り\n",
    "  * 自分とスイカの相対姿勢がはっきり分からない\n",
    "  * 他者からの情報で\n",
    "* 真っ暗なビルから脱出\n",
    "  * 壁に手をついて自己位置推定\n",
    "* 実はスイカ割りで目隠しを取っても、ビルに照明がついていても、我々は正確な位置計測ができている訳ではない\n",
    "  * 結局POMDP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自己位置推定の曖昧さ\n",
    "\n",
    "* 例えばパーティクルフィルタでもカルマンフィルタでも分布は常に広がりを持つ\n",
    "* 広がっているときに平均値（と決定論的方策$\\pi$）を信じて動くとどうなるか？\n",
    "* そもそも平均値が意味を持たないこともある\n",
    "* 平均値でなくてもっといい行動の選び方はないだろうか？\n",
    "\n",
    "<img width=300 src=\"./particle_mean.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 入力が信念の方策\n",
    "\n",
    "* エージェントが$bel$から行動決定\n",
    "* 状態空間の上で定義された方策ではなく、$bel$が属する空間の上で定義された方策となる\n",
    "  * 「信念に対する方策」「信念方策」\n",
    "* どうやって求めるか？\n",
    "* そもそも求められるのか？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 問題の大きさ\n",
    "\n",
    "* 状態が100個、行動が2種類の場合\n",
    "* 有限MDP\n",
    "  * 方策のパターンは$2^{100}$通り\n",
    "  * 価値反復を使うと\n",
    "    * $O$(状態数・行動数・タスクの長さ)で方策を計算可能\n",
    "* POMDP\n",
    "  * $bel$は100個の状態上で定義される関数\n",
    "    * 無限に存在\n",
    "  * 無限にあるものに対する方策のパターンは無限\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POMDPとハンドコーディング\n",
    "\n",
    "* 例えば、パーティクルの分布から移動ロボットの行動を決めるコードを書いてみましょう\n",
    "  * 例えば脱輪したり縁石に乗り上げたりしないで、できるだけ早く角を曲がる場合を考えてみましょう\n",
    "  * （たいていの場合）ほぼ無限に場合分けできる\n",
    "    * 疲れる\n",
    "    * そもそも無理？\n",
    "* 豊富な研究テーマ\n",
    "\n",
    "<img width=\"300\" src=\"corner-300x110.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 対策\n",
    "\n",
    "* センサを強化\n",
    "* 観測戦略\n",
    "  * 位置の情報が得られそうな行動を能動的にとる\n",
    "* センサやアクチュエータの配置、ロボットの形状の工夫（身体性）\n",
    "  * ぶつかっても壊れない\n",
    "  * 情報を得やすい位置にセンサをつける\n",
    "  * 情報を得やすい/情報がなくても動けるようにアクチュエータを選定して取り付け\n",
    "* 計算でなんとか良い行動を見つける"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算でなんとか\n",
    "* だいたい以下のような方針になる\n",
    "* 方法1:\n",
    "  * $bel$の形状一個一個を状態と皆して有限MDPのように問題を解く\n",
    "* 方法2:\n",
    "  * 事前に状態が既知の有限MDPの問題を解いて$bel$からなんとか行動を得る\n",
    "  * 分布の平均値やその他代表値を1つ選んで行動決定するのもこの方法の単純な実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法1（AMDPと呼ばれる手法群）\n",
    "\n",
    "* 信念の数を有限個に近似\n",
    "* 例[Roy 99]\n",
    "  * 距離センサを持つ移動ロボットのナビゲーション問題\n",
    "  * 4次元の状態空間を作る\n",
    "    * $xy\\theta$を離散化\n",
    "    * $bel$がどれだけ曖昧か数値化して離散化\n",
    "    * 状態遷移はなんとか計算\n",
    "  * あとは価値反復等で計算\n",
    "  * 得られる行動\n",
    "    * <span style=\"color:red\">自己位置が分からなくならないように壁沿いを走る</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法2（$Q_\\text{MDP}$, 他）\n",
    "\n",
    "* 有限MDPの計算結果を利用する\n",
    "  * 状態遷移の法則性が分かっているが、ロボットが自分の状態を完全に分からない場合に使える\n",
    "* 例[Littman95]\n",
    "  * 確率密度関数$bel$と価値関数$V$から価値の期待値を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $Q_\\text{MDP}$の例\n",
    "\n",
    "* 例1: 実際のロボットの動き: https://youtu.be/fsQicKXE5AU\n",
    "* 例2: 碁盤の目の環境\n",
    "  * 1マスが1状態\n",
    "  * 行動は上下左右\n",
    "  * 同じ重みを持ったパーティクル10個が分布\n",
    "  * タスクはゴール（G）に最短歩数で到達すること\n",
    "  * 数字はコスト\n",
    "  * 灰色のところに入ろうとすると戻される\n",
    "* 問題1: 一番「価値の高い」行動は？\n",
    "\n",
    "<img width=\"300\" src=\"qmdp.png\" />\n",
    "\n",
    "* 問題2: この環境だとどうなる？\n",
    "\n",
    "<img width=\"300\" src=\"qmpd2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最近やっている研究\n",
    "\n",
    "* probabilistic flow control[Ueda 2015]\n",
    "  * 期待値計算において「重み = パーティクルの重み/価値」とする\n",
    "    * ゴールに近いパーティクルが行動決定に大きな影響を与える\n",
    "    * 投機的な行動が生成され、ロボットがゴールを探すようになる\n",
    "* ただしこれでもデッドロックは発生\n",
    "  * 研究は続く・・・\n",
    "  \n",
    "<img width=\"300\" src=\"pfc.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PFCで動くロボット\n",
    "\n",
    "* 次のような環境\n",
    "  * 距離センサでロボットが自己位置推定\n",
    "  <img width=\"300\" src=\"env_pfc.png\" />\n",
    "  * 自己位置推定が常に次のような状態\n",
    "   <img width=\"300\" src=\"pfc_robot-300x292.png\" />\n",
    "    * ほとんど推定できていない\n",
    "\n",
    "* ロボットがゴールに吸い込まれる\n",
    "  * ランダムウォークだと止まったり同じところを行ったり来たりする\n",
    "  * https://youtu.be/qs7JUygUzyI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
